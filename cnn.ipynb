{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.4.0+cu100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1) # 28 * 28 -> (28+1-5) 24 * 24\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1) # 20 * 20\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: 1 * 28 * 28\n",
    "        x = F.relu(self.conv1(x)) # 20 * 24 * 24\n",
    "        x = F.max_pool2d(x,2,2) # 12 * 12\n",
    "        x = F.relu(self.conv2(x)) # 8 * 8\n",
    "        x = F.max_pool2d(x,2,2) # 4 *4 \n",
    "        x = x.view(-1, 4*4*50) # reshape (5 * 2 * 10), view(5, 20) -> (5 * 20)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x= self.fc2(x)\n",
    "        \n",
    "        # return x\n",
    "        return F.log_softmax(x, dim=1) # log probability\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./mnist_data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data = datasets.MNIST(\"./mnist_data\", train=True, download=True,\n",
    "                           transform=transforms.Compose([transforms.ToTensor(),]))\n",
    "mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data[5][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [d[0].data.cpu().numpy() for d in mnist_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30810776"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13066062"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data[230][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        pred = model(data) # batch_size * 10\n",
    "        loss = F.nll_loss(pred, target) \n",
    "        \n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(\"Train Epoch: {}, iteration: {}, Loss: {}\".format(\n",
    "                epoch, idx, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data) # batch_size * 10\n",
    "            total_loss += F.nll_loss(output, target, reduction=\"sum\").item() \n",
    "            pred = output.argmax(dim=1) # batch_size * 1\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "\n",
    "    total_loss /= len(test_loader.dataset)\n",
    "    acc = correct/len(test_loader.dataset) * 100.\n",
    "    print(\"Test loss: {}, Accuracy: {}\".format(total_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, iteration: 0, Loss: 2.2923946380615234\n",
      "Train Epoch: 0, iteration: 100, Loss: 0.7753957509994507\n",
      "Train Epoch: 0, iteration: 200, Loss: 0.27749717235565186\n",
      "Train Epoch: 0, iteration: 300, Loss: 0.21920061111450195\n",
      "Train Epoch: 0, iteration: 400, Loss: 0.2233399748802185\n",
      "Train Epoch: 0, iteration: 500, Loss: 0.0768377035856247\n",
      "Train Epoch: 0, iteration: 600, Loss: 0.10782098770141602\n",
      "Train Epoch: 0, iteration: 700, Loss: 0.23150363564491272\n",
      "Train Epoch: 0, iteration: 800, Loss: 0.04292915388941765\n",
      "Train Epoch: 0, iteration: 900, Loss: 0.11177463084459305\n",
      "Train Epoch: 0, iteration: 1000, Loss: 0.09185297042131424\n",
      "Train Epoch: 0, iteration: 1100, Loss: 0.16677334904670715\n",
      "Train Epoch: 0, iteration: 1200, Loss: 0.08935216069221497\n",
      "Train Epoch: 0, iteration: 1300, Loss: 0.01590600609779358\n",
      "Train Epoch: 0, iteration: 1400, Loss: 0.14203670620918274\n",
      "Train Epoch: 0, iteration: 1500, Loss: 0.058041635900735855\n",
      "Train Epoch: 0, iteration: 1600, Loss: 0.12018963694572449\n",
      "Train Epoch: 0, iteration: 1700, Loss: 0.011225099675357342\n",
      "Train Epoch: 0, iteration: 1800, Loss: 0.2052678018808365\n",
      "Test loss: 0.06720616901926696, Accuracy: 97.92999999999999\n",
      "Train Epoch: 1, iteration: 0, Loss: 0.01515431422740221\n",
      "Train Epoch: 1, iteration: 100, Loss: 0.047601234167814255\n",
      "Train Epoch: 1, iteration: 200, Loss: 0.008578703738749027\n",
      "Train Epoch: 1, iteration: 300, Loss: 0.06383751332759857\n",
      "Train Epoch: 1, iteration: 400, Loss: 0.1590208113193512\n",
      "Train Epoch: 1, iteration: 500, Loss: 0.037498604506254196\n",
      "Train Epoch: 1, iteration: 600, Loss: 0.049560777842998505\n",
      "Train Epoch: 1, iteration: 700, Loss: 0.09380640834569931\n",
      "Train Epoch: 1, iteration: 800, Loss: 0.015538929961621761\n",
      "Train Epoch: 1, iteration: 900, Loss: 0.021057825535535812\n",
      "Train Epoch: 1, iteration: 1000, Loss: 0.01478884182870388\n",
      "Train Epoch: 1, iteration: 1100, Loss: 0.02215639501810074\n",
      "Train Epoch: 1, iteration: 1200, Loss: 0.024919450283050537\n",
      "Train Epoch: 1, iteration: 1300, Loss: 0.030262483283877373\n",
      "Train Epoch: 1, iteration: 1400, Loss: 0.020378757268190384\n",
      "Train Epoch: 1, iteration: 1500, Loss: 0.06149180606007576\n",
      "Train Epoch: 1, iteration: 1600, Loss: 0.01311257854104042\n",
      "Train Epoch: 1, iteration: 1700, Loss: 0.036210209131240845\n",
      "Train Epoch: 1, iteration: 1800, Loss: 0.09771452099084854\n",
      "Test loss: 0.03821084991879761, Accuracy: 98.77\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", train=True, download=True,\n",
    "           transform=transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               transforms.Normalize((0.1307,), (0.3081,))\n",
    "           ])),\n",
    "    batch_size=batch_size, shuffle=True, \n",
    "    num_workers=1, pin_memory=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", train=False, download=True,\n",
    "           transform=transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               transforms.Normalize((0.1307,), (0.3081,))\n",
    "           ])),\n",
    "    batch_size=batch_size, shuffle=True, \n",
    "    num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "lr = 0.01\n",
    "momentum  = 0.5\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "    \n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
